{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import ipyplot\n",
    "\n",
    "all_images = list(\n",
    "    Path(\"../output/bdhl_flickr_downloads/72157719480387299/\").rglob(\"*.jpg\")\n",
    ")\n",
    "ipyplot.plot_images([str(e) for e in all_images], max_images=10, img_width=200)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive CV2 thresholding\n",
    "- Patchy outputs > too much fiddliness with thresholding for per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via https://stackoverflow.com/questions/64491530/how-to-remove-the-background-from-a-picture-in-opencv-python\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background_white(img_path):\n",
    "    img = cv2.imread(str(img_path))\n",
    "    lower_thresh = 120\n",
    "    lower = np.array([lower_thresh, lower_thresh, lower_thresh])\n",
    "    upper = np.array([255, 255, 255])\n",
    "\n",
    "    # select everything within thresh (colours idealy)\n",
    "    thresh = cv2.inRange(img, lower, upper)\n",
    "\n",
    "    # smooth mask morphology\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = 255 - morph\n",
    "\n",
    "    # apply mask to image\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "    return thresh\n",
    "\n",
    "\n",
    "# ipyplot.plot_images([str(e) for e in all_images], max_images=10, img_width=150)\n",
    "ipyplot.plot_images(\n",
    "    [remove_background_white(e) for e in all_images], max_images=10, img_width=200\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Skimage filtering algorithms\n",
    "- https://scikit-image.org/docs/stable/auto_examples/\n",
    "- helpful crash course notes: https://scikit-image.org/docs/stable/user_guide/numpy_images.html\n",
    "- transparent mask: https://stackoverflow.com/questions/62813546/how-do-i-crop-an-image-based-on-custom-mask-in-python\n",
    "- morphological transformations: https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\n",
    "- masks are pretty good? but still contain noise/partial masking of subjects\n",
    "- create contours > per subject image extraction > lots of contours aren't closed; issues with filling in the contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a decent filter\n",
    "from skimage.filters import try_all_threshold\n",
    "\n",
    "for image in all_images[:10]:\n",
    "    image = cv2.imread(str(image))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    fig, ax = try_all_threshold(gray, figsize=(12, 12), verbose=False)\n",
    "    plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find largest things in image > proportion of image size > split into individual masks > save individuals\n",
    "# fill inside largest outline?\n",
    "# find all contours > thresh?\n",
    "\n",
    "# try and remove small objects? https://www.kite.com/blog/python/image-segmentation-tutorial/\n",
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "ipyplot.plot_images([mask, remove_small_objects(mask)], max_images=20, img_width=400)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from skimage.filters import (\n",
    "    threshold_mean,\n",
    "    threshold_otsu,\n",
    "    threshold_triangle,\n",
    "    threshold_yen,\n",
    ")\n",
    "\n",
    "ipyplot_img_with = 500\n",
    "\n",
    "\n",
    "def get_threshold_mask(cv2_image, log=True):\n",
    "    # creat image mask\n",
    "    gray = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2GRAY)\n",
    "    mask = gray > threshold_mean(gray)  # yen, triangle or mean?\n",
    "    gray[mask] = 0\n",
    "\n",
    "    if log:\n",
    "        ipyplot.plot_images(\n",
    "            [mask.astype(np.uint8), gray], max_images=20, img_width=ipyplot_img_with\n",
    "        )\n",
    "    return gray, mask\n",
    "\n",
    "\n",
    "def apply_mask(image, mask, save_dir, image_name, log=True):\n",
    "    # apply mask\n",
    "    mask_2 = mask.astype(np.uint8)\n",
    "    mask_2 = (~mask_2.astype(bool)).astype(np.uint8)\n",
    "\n",
    "    # smooth morphology\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    mask_2 = cv2.morphologyEx(mask_2, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # apply mask\n",
    "    masked = cv2.bitwise_and(image, image, mask=mask_2)\n",
    "    if log:\n",
    "        ipyplot.plot_images([masked], max_images=20, img_width=ipyplot_img_with)\n",
    "\n",
    "    # if save_dir:\n",
    "    #     cv2.imwrite(str(save_dir / f\"{image_name}_mask.jpeg\"), masked)\n",
    "    return masked\n",
    "\n",
    "\n",
    "def remove_background(masked_image, save_dir, image_name, log=True):\n",
    "    # save with transparent background\n",
    "    tmp = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)\n",
    "    _, alpha = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY)\n",
    "    b, g, r = cv2.split(masked_image)\n",
    "    rgba = [b, g, r, alpha]\n",
    "    masked_tr = cv2.merge(rgba, 4)\n",
    "\n",
    "    if log:\n",
    "        ipyplot.plot_images([masked_tr], max_images=20, img_width=ipyplot_img_with)\n",
    "\n",
    "    if save_dir:\n",
    "        cv2.imwrite(str(save_dir / f\"{image_name}_mask.png\"), masked_tr)\n",
    "\n",
    "    return masked_tr\n",
    "\n",
    "\n",
    "save_dir = Path(\"../output/filter_segmentation\")\n",
    "shutil.rmtree(str(save_dir)) if save_dir.exists() else None\n",
    "save_dir.mkdir()\n",
    "for image_file in all_images:\n",
    "    image = cv2.imread(str(image_file))\n",
    "    gray, mask = get_threshold_mask(image, log=False)\n",
    "    # mask = ndi.binary_fill_holes(mask)\n",
    "    masked = apply_mask(image, mask, save_dir, image_file.stem, log=False)\n",
    "    masked_bg_removed = remove_background(masked, save_dir, image_file.stem, log=False)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skimage segmentation tutorial\n",
    "- https://scikit-image.org/docs/stable/user_guide/tutorial_segmentation.html?highlight=contour\n",
    "- focus on using canny edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.exposure import histogram\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import sobel\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "plt_figsize = (10, 10)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to gray\n",
    "image = cv2.imread(str(all_images[2]))\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray, mask = get_threshold_mask(image, log=False)\n",
    "# gray = rgb2gray(image)\n",
    "# enhance = cv2.equalizeHist(gray) # probably not; terrible chunking effect\n",
    "\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # luminescence values?\n",
    "# gray_sk = rgb2gray(image)\n",
    "# gray\n",
    "# gray_sk\n",
    "\n",
    "# get edges\n",
    "input_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = canny(input_img / 255.0)\n",
    "edges_2 = edges.astype(np.uint8)\n",
    "# plt.figure(figsize=plt_figsize)\n",
    "# plt.imshow(edges_2)\n",
    "\n",
    "# fill inside of edges - binary fill\n",
    "fill_edges = ndi.binary_fill_holes(\n",
    "    edges_2, structure=cv2.getStructuringElement(cv2.MORPH_OPEN, (20, 20))\n",
    ")\n",
    "# plt.figure(figsize=plt_figsize)\n",
    "# plt.imshow(fill_edges)\n",
    "\n",
    "elevation_map = sobel(gray)\n",
    "# plt.figure(figsize=plt_figsize)\n",
    "# plt.imshow(elevation_map)\n",
    "\n",
    "markers = np.zeros_like(gray)\n",
    "markers[gray < 20] = 1\n",
    "markers[gray > 50] = 2\n",
    "\n",
    "segmentation = watershed(elevation_map, markers)\n",
    "\n",
    "plt.figure(figsize=plt_figsize)\n",
    "plt.imsave(\"./temp_dir/temp.jpeg\", segmentation)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HED edge detection\n",
    "- DL based instead? > semantic understanding of image\n",
    "- https://pyimagesearch.com/2019/03/04/holistically-nested-edge-detection-with-opencv-and-deep-learning/\n",
    "- https://cv-tricks.com/opencv-dnn/edge-detection-hed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply low-level segmentation?\n",
    "- https://scikit-image.org/docs/stable/auto_examples/edges/plot_active_contours.html#sphx-glr-auto-examples-edges-plot-active-contours-py\n",
    "- Not effective; outlines of sub-segments still spill/chaotic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import (\n",
    "    felzenszwalb,\n",
    "    mark_boundaries,\n",
    "    quickshift,\n",
    "    slic,\n",
    "    watershed,\n",
    ")\n",
    "\n",
    "# image = cv2.imread(str(all_images[2]))\n",
    "\n",
    "segments_fz = felzenszwalb(masked, scale=100, sigma=0.5, min_size=50)\n",
    "segments_slic = slic(masked, n_segments=250, compactness=10, sigma=1, start_label=1)\n",
    "segments_quick = quickshift(masked, kernel_size=3, max_dist=6, ratio=0.5)\n",
    "gradient = sobel(rgb2gray(masked))\n",
    "segments_watershed = watershed(gradient, markers=250, compactness=0.001)\n",
    "\n",
    "plt.figure(figsize=plt_figsize)\n",
    "plt.imshow(mark_boundaries(image, segments_fz))\n",
    "\n",
    "plt.figure(figsize=plt_figsize)\n",
    "plt.imshow(mark_boundaries(image, segments_slic))\n",
    "\n",
    "# plt.figure(figsize=plt_figsize)\n",
    "# plt.imshow(mark_boundaries(image, gradient))\n",
    "\n",
    "plt.figure(figsize=plt_figsize)\n",
    "plt.imshow(mark_boundaries(image, segments_watershed))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find contours\n",
    "- https://scikit-image.org/docs/stable/auto_examples/edges/plot_contours.html\n",
    "- issues with contour selection? often contours the entire image after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "# constant value not suitable for inputs\n",
    "contours = measure.find_contours(gray, 0.8)\n",
    "\n",
    "# Display the image and plot all contours found\n",
    "plt.figure(figsize=plt_figsize)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(gray, cmap=plt.cm.gray)\n",
    "\n",
    "for contour in contours:\n",
    "    ax.plot(contour[:, 1], contour[:, 0], linewidth=2)\n",
    "\n",
    "ax.axis(\"image\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/67285972/how-to-fill-canny-edge-image-in-opencv-python\n",
    "contours = cv2.findContours(\n",
    "    mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    ")\n",
    "contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "\n",
    "min_contour_size = 0.1\n",
    "filtered_contours = [\n",
    "    e for e in contours if (cv2.contourArea(e) / masked.size) > min_contour_size\n",
    "]\n",
    "\n",
    "# large-ish contours are the same size as the image?\n",
    "gray.size\n",
    "cv2.contourArea(filtered_contours[0])\n",
    "\n",
    "# # draw white filled contour on black background\n",
    "# result = np.zeros_like(gray)\n",
    "# cv2.drawContours(result, [big_contour], 0, (255,255,255), cv2.FILLED)\n",
    "\n",
    "# ipyplot.plot_images([gray], max_images=20, img_width=400)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://betterprogramming.pub/image-segmentation-python-7a838a464a84\n",
    "# temp = cv2.cvtColor(masked, cv2.COLOR_BGR2GRAY)\n",
    "contours = cv2.findContours(\n",
    "    mask.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n",
    ")[-2]\n",
    "\n",
    "min_contour_size = 0.1\n",
    "filtered_contours = [\n",
    "    e for e in contours if (cv2.contourArea(e) / masked.size) > min_contour_size\n",
    "]\n",
    "\n",
    "cv2.drawContours(gray, filtered_contours, -1, (0, 255, 0), 1)\n",
    "ipyplot.plot_images([gray], max_images=20, img_width=400)\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
