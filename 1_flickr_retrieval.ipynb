{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flickrapi\n",
    "!pip list | grep 'flickr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flickrapi\n",
    "\n",
    "\n",
    "def get_photo_url_etree(photo_id):\n",
    "    sizes = flickr.photos.getSizes(photo_id=photo_id)\n",
    "    largest_available_size = (\n",
    "        pd.DataFrame([dict(e.items()) for e in list(sizes.find(\"sizes\"))])\n",
    "        .sort_values(by=[\"width\", \"height\"], ascending=False)\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    return largest_available_size.to_dict()\n",
    "\n",
    "\n",
    "def retrieve_photo_meta_data(album_id):\n",
    "    photo_records = []\n",
    "    try:\n",
    "        photos_raw = list(flickr.walk_set(album_id))\n",
    "    except:\n",
    "        logging.error(f\"Unable to walk photos for album: {album_id}\")\n",
    "        return pd.DataFrame()  # cat empty df all the same? bit yikes\n",
    "\n",
    "    for photo in tqdm(\n",
    "        photos_raw, desc=f\"Retrieving photo meta data for album: {album_id}\"\n",
    "    ):\n",
    "        photo = dict(photo.items())  # silly e-tree format\n",
    "        try:\n",
    "            largest_size = get_photo_url_etree(photo[\"id\"])\n",
    "            photo[\"photo_meta\"] = largest_size\n",
    "            photo_records.append(photo)\n",
    "        except:\n",
    "            logging.error(f\"Unable to retrieve photo size for: {photo['id']}\")\n",
    "\n",
    "    photos = (\n",
    "        pd.DataFrame(photo_records)\n",
    "        .assign(album_id=album_id)\n",
    "        .assign(download_url=lambda x: x.photo_meta.apply(lambda y: y[\"source\"]))\n",
    "        # filter out small images\n",
    "        .assign(width=lambda x: x.photo_meta.apply(lambda y: int(y[\"width\"])))\n",
    "        .assign(height=lambda x: x.photo_meta.apply(lambda y: int(y[\"height\"])))\n",
    "        .query(\"height >= 256 & width >= 256\")\n",
    "        .pipe(\n",
    "            lambda x: x.sample(n=n_photos_per_album, random_state=42)\n",
    "            if x.shape[0] > n_photos_per_album\n",
    "            else x\n",
    "        )\n",
    "    )\n",
    "    return photos\n",
    "\n",
    "\n",
    "def download_flickr_image(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "    res = requests.get(url, stream=True)\n",
    "\n",
    "\n",
    "def download_photo_record(record, download_dir):\n",
    "    # mkdir album save dir if doesn't exist\n",
    "    if (download_dir / record.album_id).exists() == False:\n",
    "        (download_dir / record.album_id).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = f\"{(download_dir / record.album_id / record.id).as_posix()}{Path(record.download_url).suffix}\"\n",
    "    print(save_path)\n",
    "    if Path(save_path).exists() == True:\n",
    "        logging.info(f\"Previously saved: {save_path}; skipping\")\n",
    "    else:\n",
    "        try:\n",
    "            download_flickr_image(record.download_url, save_path)\n",
    "        except Exception:\n",
    "            logging.error(f\"Unable to download image at: {record.download_url}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some real fucked auth\n",
    "api_key = \"4084101d41128cb58d7ff6607cbf4fe0\"\n",
    "api_secret = \"0eccc48b5d59596b\"\n",
    "\n",
    "# e tree for user retrievals, json for favourite retrievals**\n",
    "flickr = flickrapi.FlickrAPI(api_key, api_secret, format=\"etree\")\n",
    "flickr.authenticate_console()  # 401 error anyway? but still works?\n",
    "# flickr.authenticate_via_browser(perms='write')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_albums = 1\n",
    "n_photos_per_album = 10\n",
    "user_id = \"61021753@N02\"\n",
    "\n",
    "# retrieve some biodiversity albums\n",
    "bdhl = flickr.photosets.getList(user_id=user_id, page=1)  # paginated\n",
    "bdhl_df = pd.DataFrame([dict(e.items()) for e in list(bdhl.find(\"photosets\"))]).sample(\n",
    "    n=n_albums, random_state=42\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_albums = [\n",
    "    \"https://www.flickr.com/photos/biodivlibrary/albums/72157719480387299\",\n",
    "    \"https://www.flickr.com/photos/biodivlibrary/albums/72157719491069662\",\n",
    "    \"https://www.flickr.com/photos/biodivlibrary/albums/72157719533382815\",\n",
    "    \"https://www.flickr.com/photos/biodivlibrary/albums/72157719532261290\",\n",
    "    \"https://www.flickr.com/photos/biodivlibrary/albums/72157719532226635\",\n",
    "]\n",
    "curated_albums = [Path(e).name for e in curated_albums]\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for album in curated_albums:\n",
    "#     all_photos.append(retrieve_photo_meta_data(album))\n",
    "# all_photos = pd.concat(all_photos)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk the albums, retrieve individual photo details\n",
    "all_photos = []\n",
    "for idx, album in bdhl_df.iterrows():\n",
    "    all_photos.append(retrieve_photo_meta_data(album.id))\n",
    "\n",
    "# download each photo\n",
    "download_dir = Path(\"./biodiversity_library\")\n",
    "download_dir.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ") if download_dir.exists() == False else None\n",
    "\n",
    "bio_diversity_all = all_photos.apply(\n",
    "    lambda y: download_photo_record(y, download_dir), axis=1\n",
    ")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
